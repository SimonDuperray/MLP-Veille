# -*- coding: utf-8 -*-
"""MLP-Veille.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QBacT7nd-B1rs99TzNurpBPfwuqKIlsu
"""

!pip install bayesian_optimization
!pip install keras_tuner

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow import keras
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from keras.wrappers.scikit_learn import KerasClassifier
from bayes_opt import BayesianOptimization
from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials

import time
import keras_tuner as kt
import pandas as pd
import numpy as np
import tensorflow as tf

np.set_printoptions(suppress=True)
tf.__version__

titanic = pd.read_pickle('./TitanicSurvivalDataNumeric.pkl')
titanic.head()

"""<ul>
<li>survived</li>
<li>Pclass: travel class passenger</li>
<li>sex</li>
<li>age</li>
<li>SibSp: number of siblings/spouses aboard</li>
<li>Parch: number of parents/children aboard</li>
<li>Fare: the amount of fare paid by the passenger</li>
<li>Embarked_C: Cherbourg</li>
<li>Embarked_Q: Queenstown</li>
<li>Embarked_S: Southampton</il>
<ul>
"""

titanic.info()

target = ['Survived']
predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_C', 'Embarked_Q', 'Embarked_S']

X = titanic[predictors].values
y = titanic[target].values

predictor_scaler = StandardScaler()
predictor_scaler_fit = predictor_scaler.fit(X)

X = predictor_scaler_fit.transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

def init_cls():
  start_time = time.time()
  classifier = tf.keras.models.Sequential()
  classifier.add(tf.keras.layers.Dense(units=10, input_dim=9, kernel_initializer='uniform', activation='relu'))
  classifier.add(tf.keras.layers.Dense(units=6, kernel_initializer='uniform', activation='relu'))
  classifier.add(tf.keras.layers.Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))

  classifier.compile(optimizer="adam", loss="binary_crossentropy", metrics=['accuracy'])

  survival_model = classifier.fit(X_train, y_train, batch_size=10, epochs=10, verbose=1)
  end_time = time.time()
  print(f"exec time: {round((end_time-start_time)/60, 2)}")
  classifier.summary()

init_cls()

def find_best_params(X_train, y_train):
  trial_number = 0

  search_result_data = pd.DataFrame(columns=['TrialNumber', 'Parameters', 'Accuracy'])

  for lr_trial in HP_RANGES['lr']:
    for optimizer_trial in HP_RANGES['optimizer']:
      for epochs_trial in HP_RANGES['epochs']:
        for batch_size_trial in HP_RANGES['batch_size']:
          trial_number += 1

          classifier = tf.keras.models.Sequential()
          classifier.add(tf.keras.layers.Dense(units=10, input_dim=9, kernel_initializer='uniform', activation='relu'))
          classifier.add(tf.keras.layers.Dense(units=6, kernel_initializer='uniform', activation='relu'))
          classifier.add(tf.keras.layers.Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))
          classifier.compile(optimizer=optimizer_trial, loss='binary_crossentropy', metrics=['accuracy'])

          survival = classifier.fit(X_train, y_train, batch_size=batch_size_trial, epochs=epochs_trial, verbose=0)
          accuracy = survival.history['accuracy'][-1]

          print(trial_number, 'parameters:', 'batch_size', batch_size_trial,'-','epochs:',epochs_trial,'accuracy:',accuracy)
          search_result_data = search_result_data.append(pd.DataFrame(data=[[trial_number,
                                'batch_size'+str(batch_size_trial)+'-'+'epoch'+str(epochs_trial), accuracy]],
                                                                        columns=['TrialNumber', 'Parameters', 'Accuracy'] ))
      
  return search_result_data

results = find_best_params(X_train, y_train)

print(results.sort_values(by='Accuracy', ascending=False).head(1))
results.plot(x='Parameters', y='Accuracy', figsize=(15, 4), rot=20)

def make_classification_ann(optimizer_trial=None, neurons_trial=None):
  classifier = tf.keras.models.Sequential()
  classifier.add(tf.keras.layers.Dense(units=10, input_dim=9, kernel_initializer='uniform', activation='relu'))
  classifier.add(tf.keras.layers.Dense(units=6, kernel_initializer='uniform', activation='relu'))
  classifier.add(tf.keras.layers.Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))
  classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
  return classifier

# GRID SEARCH CV

parameters_trial = {
    "batch_size": [10, 20, 30],
    "epochs": [10, 20, 50],
    "optimizer_trial": ['adam', 'rmsprop'],
    "neurons_trial":  [5, 10, 20, 50]
}

classifier_model = KerasClassifier(make_classification_ann, verbose=0)
grid_search = GridSearchCV(estimator=classifier_model, param_grid=parameters_trial, scoring='f1', cv=5)

start_time = time.time()

grid_search.fit(X_train, y_train, verbose=1)

end_time = time.time()
print(f"######## Total time takenn: {round((end_time-start_time)/60, 2)} minutes ######")

print("\n### Best hyperparameters ###")
grid_search.best_params_

def test_grid_search():
  classifier = tf.keras.models.Sequential()
  classifier.add(tf.keras.layers.Dense(units=20, input_dim=9, kernel_initializer='uniform', activation='relu'))
  classifier.add(tf.keras.layers.Dense(units=20, kernel_initializer='uniform', activation='relu'))
  classifier.add(tf.keras.layers.Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))

  classifier.compile(optimizer="rmsprop", loss="binary_crossentropy", metrics=['accuracy'])

  survival_model = classifier.fit(X_train, y_train, batch_size=10, epochs=20, verbose=1)
  classifier.summary()

test_grid_search()

# RANDOMIZED SEARCH CV

parameters_trial = {
    "batch_size": [10, 20, 30],
    "epochs": [10, 20, 50],
    "optimizer_trial": ['adam', 'rmsprop'],
    "neurons_trial":  [5, 10, 20, 50]
}

classifier_model = KerasClassifier(make_classification_ann, verbose=0)
grid_search = RandomizedSearchCV(estimator=classifier_model, param_distributions=parameters_trial, scoring='f1', cv=5)

start_time = time.time()

grid_search.fit(X_train, y_train, verbose=1)

end_time = time.time()
print(f"######## Total time takenn: {round((end_time-start_time)/60, 2)} minutes ######")

print("\n### Best hyperparameters ###")
grid_search.best_params_

def test_random_grid_search():
  classifier = tf.keras.models.Sequential()
  classifier.add(tf.keras.layers.Dense(units=20, input_dim=9, kernel_initializer='uniform', activation='relu'))
  classifier.add(tf.keras.layers.Dense(units=20, kernel_initializer='uniform', activation='relu'))
  classifier.add(tf.keras.layers.Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))

  classifier.compile(optimizer="adam", loss="binary_crossentropy", metrics=['accuracy'])

  survival_model = classifier.fit(X_train, y_train, batch_size=10, epochs=10, verbose=1)
  classifier.summary()

test_random_grid_search()

d = hp.choice('units', [5, 10])
type(d)

# BAYESIAN OPTIMIZATION

batch_size_ranges = [10, 20, 30]
epochs_ranges = [10, 20, 50]
optimizers = ['adam', 'rmsprop']
units_ranges = [5, 10, 20, 50]

space = {
    'units': hp.choice('units', units_ranges),
    'batch_size': hp.choice('batch_size', batch_size_ranges),
    'epochs': hp.choice('epochs', epochs_ranges),
    'optimizer': hp.choice('optimizer', optimizers),
}

def f_nn(params):
  classifier = tf.keras.models.Sequential()
  classifier.add(tf.keras.layers.Dense(units=params['units'], input_dim=9, kernel_initializer='uniform', activation='relu'))
  classifier.add(tf.keras.layers.Dense(units=params['units'], kernel_initializer='uniform', activation='relu'))
  classifier.add(tf.keras.layers.Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))

  classifier.compile(optimizer=params['optimizer'], loss="binary_crossentropy", metrics=['accuracy'])

  survival_model = classifier.fit(X_train, y_train, batch_size=params['batch_size'], epochs=params['epochs'], verbose=0)
  acc = survival_model.history['accuracy'][-1]
  print(f"Got AUC={acc} with: units={params['units']} - optimizer={params['optimizer']} - batch_size={params['batch_size']} - epochs={params['epochs']}")
  return {'loss': 1-acc, 'status': STATUS_OK }

trials = Trials()
start_time = time.time()
best = fmin(f_nn, space, algo=tpe.suggest, max_evals=50, trials=trials)
end_time = time.time()
print(f"Best params found:\n{best} in {round((end_time-start_time)/60, 2)}")

def test_random_grid_search():
  classifier = tf.keras.models.Sequential()
  classifier.add(tf.keras.layers.Dense(units=50, input_dim=9, kernel_initializer='uniform', activation='relu'))
  classifier.add(tf.keras.layers.Dense(units=50, kernel_initializer='uniform', activation='relu'))
  classifier.add(tf.keras.layers.Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))

  classifier.compile(optimizer="adam", loss="binary_crossentropy", metrics=['accuracy'])

  survival_model = classifier.fit(X_train, y_train, batch_size=20, epochs=50, verbose=0)
  print(survival_model.history['accuracy'][-1])

for _ in range(10):
  test_random_grid_search()